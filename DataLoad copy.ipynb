{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kh-C8E00Nic0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downlading data... Done\n"
          ]
        }
      ],
      "source": [
        "# Download data\n",
        "print('Downlading data... ', end='')\n",
        "# Your code here\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing Libraries... Done\n"
          ]
        }
      ],
      "source": [
        "# Importing Libraries\n",
        "print('Importing Libraries... ',end='')\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import zipfile\n",
        "from torchaudio.transforms import Resample\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FHoVq9fM7EHL"
      },
      "outputs": [],
      "source": [
        "# # Extract data\n",
        "# with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_SDQQJfNK-4"
      },
      "outputs": [],
      "source": [
        "# Getting list of raw audio files\n",
        "wavs = list(path.glob('audio/*'))  # List all audio files in the 'audio' directory using pathlib.Path.glob\n",
        "\n",
        "# Visualizing data\n",
        "waveform, sample_rate = torchaudio.load(wavs[0])  # Load the waveform and sample rate of the first audio file using torchaudio\n",
        "\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))  # Print the shape of the waveform tensor\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))  # Print the sample rate of the audio file\n",
        "\n",
        "# Plot the waveform using matplotlib\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())  # Transpose and convert the waveform tensor to a NumPy array for plotting\n",
        "\n",
        "# Display the audio using IPython.display.Audio\n",
        "ipd.Audio(waveform, rate=sample_rate)  # Create an interactive audio player for the loaded waveform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        # Initialize CustomDataset object with relevant parameters\n",
        "        # dataset: \"train\", \"val\", or \"test\"\n",
        "        # kwargs: Additional parameters like data directory, dataframe, folds, etc.\n",
        "\n",
        "        # Extract parameters from kwargs\n",
        "        self.data_directory = kwargs[\"data_directory\"]\n",
        "        self.data_frame = kwargs[\"data_frame\"]\n",
        "        self.validation_fold = kwargs[\"validation_fold\"]\n",
        "        self.testing_fold = kwargs[\"testing_fold\"]\n",
        "        self.esc_10_flag = kwargs[\"esc_10_flag\"]\n",
        "        self.file_column = kwargs[\"file_column\"]\n",
        "        self.label_column = kwargs[\"label_column\"]\n",
        "        self.sampling_rate = kwargs[\"sampling_rate\"]\n",
        "        self.new_sampling_rate = kwargs[\"new_sampling_rate\"]\n",
        "        self.sample_length_seconds = kwargs[\"sample_length_seconds\"]\n",
        "\n",
        "        # Filter dataframe based on esc_10_flag and data_type\n",
        "        if self.esc_10_flag:\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['esc10'] == True]\n",
        "\n",
        "        if dataset == \"train\":\n",
        "            self.data_frame = self.data_frame.loc[\n",
        "                (self.data_frame['fold'] != self.validation_fold) & (self.data_frame['fold'] != self.testing_fold)]\n",
        "        elif dataset == \"val\":\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.validation_fold]\n",
        "        elif dataset == \"test\":\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.testing_fold]\n",
        "\n",
        "        # Get unique categories from the filtered dataframe\n",
        "        self.categories = sorted(self.data_frame[self.label_column].unique())\n",
        "\n",
        "        # Initialize lists to hold file names, labels, and folder numbers\n",
        "        self.file_names = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Initialize dictionaries for category-to-index and index-to-category mapping\n",
        "        self.category_to_index = {}\n",
        "        self.index_to_category = {}\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            self.category_to_index[category] = i\n",
        "            self.index_to_category[i] = category\n",
        "\n",
        "        # Populate file names and labels lists by iterating through the dataframe\n",
        "        for ind in tqdm(range(len(self.data_frame))):\n",
        "            row = self.data_frame.iloc[ind]\n",
        "            file_path = self.data_directory / \"audio\" / row[self.file_column]\n",
        "            self.file_names.append(file_path)\n",
        "            self.labels.append(self.category_to_index[row[self.label_column]])\n",
        "\n",
        "        self.resampler = torchaudio.transforms.Resample(self.sampling_rate, self.new_sampling_rate)\n",
        "\n",
        "        # Window size for rolling window sample splits (unfold method)\n",
        "        if self.sample_length_seconds == 2:\n",
        "            self.window_size = self.new_sampling_rate * 2\n",
        "            self.step_size = int(self.new_sampling_rate * 0.75)\n",
        "        else:\n",
        "            self.window_size = self.new_sampling_rate\n",
        "            self.step_size = int(self.new_sampling_rate * 0.5)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Split audio files with overlap, pass as stacked tensors tensor with a single label\n",
        "        path = self.file_names[index]\n",
        "        audio_file = torchaudio.load(path, format=None, normalize=True)\n",
        "        audio_tensor = self.resampler(audio_file[0])\n",
        "        splits = audio_tensor.unfold(1, self.window_size, self.step_size)\n",
        "        samples = splits.permute(1, 0, 2)\n",
        "        return samples, self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2JUJdPTRjix"
      },
      "outputs": [],
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Initialize the CustomDataModule with batch size, number of workers, and other parameters\n",
        "        super().__init__()\n",
        "        self.batch_size = kwargs[\"batch_size\"]\n",
        "        self.num_workers = kwargs[\"num_workers\"]\n",
        "        self.data_module_kwargs = kwargs\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Define datasets for training, validation, and testing during Lightning setup\n",
        "\n",
        "        # If in 'fit' or None stage, create training and validation datasets\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.training_dataset = CustomDataset(dataset=\"train\", **self.data_module_kwargs)\n",
        "            self.validation_dataset = CustomDataset(dataset=\"val\", **self.data_module_kwargs)\n",
        "\n",
        "        # If in 'test' or None stage, create testing dataset\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.testing_dataset = CustomDataset(dataset=\"test\", **self.data_module_kwargs)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Return DataLoader for training dataset\n",
        "        return DataLoader(self.training_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # Return DataLoader for validation dataset\n",
        "        return DataLoader(self.validation_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # Return DataLoader for testing dataset\n",
        "        return DataLoader(self.testing_dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def collate_function(self, data):\n",
        "        \"\"\"\n",
        "        Collate function to process a batch of examples and labels.\n",
        "\n",
        "        Args:\n",
        "            data: a tuple of 2 tuples with (example, label) where\n",
        "                example are the split 1 second sub-frame audio tensors per file\n",
        "                label = the label\n",
        "\n",
        "        Returns:\n",
        "            A list containing examples (concatenated tensors) and labels (flattened tensor).\n",
        "        \"\"\"\n",
        "        examples, labels = zip(*data)\n",
        "        # examples = torch.cat(examples)\n",
        "    \n",
        "        examples = torch.stack(examples)\n",
        "        # examples = examples.reshape(examples.size(0),-1,examples.size(-1))\n",
        "        examples = examples.reshape(examples.size(0),1,-1)\n",
        "        labels = torch.flatten(torch.tensor(labels))\n",
        "\n",
        "        return examples, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irm0dFPaS1JR"
      },
      "outputs": [],
      "source": [
        "# Data Setup\n",
        "test_samp = 1 #\"\"\" Do not change this!! \"\"\"\n",
        "valid_samp = 2 # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)\n",
        "batch_size = 32 # Free to change\n",
        "num_workers = 0 # Free to change\n",
        "custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                      num_workers=num_workers,\n",
        "                                      data_directory=path,\n",
        "                                      data_frame=df,\n",
        "                                      validation_fold=valid_samp,\n",
        "                                      testing_fold=test_samp,  # set to 0 for no test set\n",
        "                                      esc_10_flag=True,\n",
        "                                      file_column='filename',\n",
        "                                      label_column='category',\n",
        "                                      sampling_rate=44100,\n",
        "                                      new_sampling_rate=16000,  # new sample rate for input\n",
        "                                      sample_length_seconds=1  # new length of input in seconds\n",
        "                                      )\n",
        "\n",
        "custom_data_module.setup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW9GhLFjUayx"
      },
      "outputs": [],
      "source": [
        "# Data Exploration\n",
        "print('Class Label: ', custom_data_module.training_dataset[5][1])  # this prints the class label\n",
        "print('Shape of data sample tensor: ', custom_data_module.training_dataset[5][0].shape)  # this prints the shape of the sample (Frames, Channel, Features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61td-GUuUcpA"
      },
      "outputs": [],
      "source": [
        "# Dataloader(s)\n",
        "x = next(iter(custom_data_module.train_dataloader()))\n",
        "y = next(iter(custom_data_module.val_dataloader()))\n",
        "z = next(iter(custom_data_module.test_dataloader()))\n",
        "print('Train Dataloader:')\n",
        "print(torch.tensor(x[0]).shape, len(x[1]))\n",
        "print('Validation Dataloader:')\n",
        "print(torch.tensor(y[0]).shape, len(y[1]))\n",
        "print('Test Dataloader:')\n",
        "print(torch.tensor(z[0]).shape, len(z[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = custom_data_module.train_dataloader()\n",
        "valid_loader = custom_data_module.val_dataloader()\n",
        "test_loader = custom_data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=11,stride=1,padding='valid')\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=9, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
        "        # self.conv5 = nn.Conv1d(in_channels=256, out_channels=64, kernel_size=3, stride=2)\n",
        "        # self.conv6 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        # self.conv7 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=8192, out_features=1024)\n",
        "        # self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=n_classes)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(32,affine=False)\n",
        "        self.bn2 = nn.BatchNorm1d(64,affine=False)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        \n",
        "        # y = x.clone()\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=16,stride=1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv3(x)\n",
        "        # x = x + y\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=4,stride=1)\n",
        "        x = self.conv4(x)\n",
        "        x = F.adaptive_avg_pool1d(x,128)\n",
        "        x = self.bn2(x)\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv5(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv6(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv7(x)\n",
        "\n",
        "        # x = F.avg_pool1d(x,kernel_size=2,stride=1)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, train_loader,valid_loader, optimizer, criterion, device,epochs = 10):\n",
        "\n",
        "    history = {'train':{'loss':[],'accuracy':[]}, 'valid':{'loss':[],'accuracy':[]}}\n",
        "    n = len(train_loader)\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        Loss_epoch = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        evaluation_train = {'accuracy':0, 'loss':0}\n",
        "        for idx, data in enumerate(tqdm(train_loader)):\n",
        "            input, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            Loss_epoch += loss.item()*len(labels)\n",
        "            correct += accuracy(output, labels)*len(labels)\n",
        "            total += len(labels)\n",
        "        # Loss_history.append(Loss_epoch/n)\n",
        "        evaluation_train['accuracy'] = correct/total\n",
        "        evaluation_train['loss'] =  Loss_epoch/total\n",
        "        # evaluation_train = evaluate(model, train_loader,criterion, device)\n",
        "        evaluation_valid,_ = evaluate(model, valid_loader,criterion, device)\n",
        "        print(f'train: {evaluation_train}, valid: {evaluation_valid}')\n",
        "        history['train']['accuracy'].append(evaluation_train['accuracy'])\n",
        "        history['train']['loss'].append(evaluation_train['loss'])\n",
        "\n",
        "\n",
        "\n",
        "        history['valid']['loss'].append(evaluation_valid['loss'])\n",
        "        history['valid']['accuracy'].append(evaluation_valid['accuracy'])\n",
        "    return history\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    _, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
        "\n",
        "def evaluate(model,data_loader, criterion, device,return_preds = False):\n",
        "    model.eval()\n",
        "    Accuracy_history = []\n",
        "    Loss_history = []\n",
        "    PREDS = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            input, target = data[0].to(device), data[1].to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            Accuracy_history.append(accuracy(output, target))\n",
        "            Loss_history.append(loss.item())\n",
        "            if return_preds:\n",
        "                PREDS.extend(torch.max(output, dim=1)[1].tolist())\n",
        "    return {'accuracy': torch.mean(torch.Tensor(Accuracy_history)).item(), 'loss': torch.mean(torch.Tensor(Loss_history)).item()}, PREDS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def numel(m: torch.nn.Module, only_trainable: bool = False):\n",
        "    \"\"\"\n",
        "    Returns the total number of parameters used by `m` (only counting\n",
        "    shared parameters once); if `only_trainable` is True, then only\n",
        "    includes parameters with `requires_grad = True`\n",
        "    \"\"\"\n",
        "    parameters = list(m.parameters())\n",
        "    if only_trainable:\n",
        "        parameters = [p for p in parameters if p.requires_grad]\n",
        "    unique = {p.data_ptr(): p for p in parameters}.values()\n",
        "    return sum(p.numel() for p in unique)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numel(model(10),only_trainable=True),numel(model(10),only_trainable=False)-numel(model(10),only_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "n_classes = len(custom_data_module.training_dataset.categories)# number of classes = 10\n",
        "model_train = model(n_classes)\n",
        "history = train(model_train, train_loader, valid_loader, optim.Adam(model_train.parameters(), lr=0.001), nn.CrossEntropyLoss(), 'cuda', epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ques2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4D2_W9OLZON"
      },
      "source": [
        "This code is useful in data loading, visualization and exploration. You are free to modify the code. The code has dependecy on Pytorch Lightning data module. However, you may use Pytorch as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOr0uxaMG_u"
      },
      "source": [
        "**Introduction to Dataset**\n",
        "\n",
        "The data has a total of 10 classes with 40 samples each. Make sure while working with the data, **esc10=True**. In the assignment, you are required to perform 4-fold validation. This dataset has been already divided into 5-folds. The column 'fold' in the metafile denotes the sample in a particular fold. Moreover, first folds is considered for test, rest for 4-fold validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "print('Importing Libraries... ',end='')\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import zipfile\n",
        "from torchaudio.transforms import Resample\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_SDQQJfNK-4"
      },
      "outputs": [],
      "source": [
        "# Getting list of raw audio files\n",
        "wavs = list(path.glob('audio/*'))  # List all audio files in the 'audio' directory using pathlib.Path.glob\n",
        "\n",
        "# Visualizing data\n",
        "waveform, sample_rate = torchaudio.load(wavs[0])  # Load the waveform and sample rate of the first audio file using torchaudio\n",
        "\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))  # Print the shape of the waveform tensor\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))  # Print the sample rate of the audio file\n",
        "\n",
        "# Plot the waveform using matplotlib\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())  # Transpose and convert the waveform tensor to a NumPy array for plotting\n",
        "\n",
        "# Display the audio using IPython.display.Audio\n",
        "ipd.Audio(waveform, rate=sample_rate)  # Create an interactive audio player for the loaded waveform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irm0dFPaS1JR"
      },
      "outputs": [],
      "source": [
        "# Data Setup\n",
        "test_samp = 1 #\"\"\" Do not change this!! \"\"\"\n",
        "valid_samp = 2 # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)\n",
        "batch_size = 32 # Free to change\n",
        "num_workers = 0 # Free to change\n",
        "custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                      num_workers=num_workers,\n",
        "                                      data_directory=path,\n",
        "                                      data_frame=df,\n",
        "                                      validation_fold=valid_samp,\n",
        "                                      testing_fold=test_samp,  # set to 0 for no test set\n",
        "                                      esc_10_flag=True,\n",
        "                                      file_column='filename',\n",
        "                                      label_column='category',\n",
        "                                      sampling_rate=44100,\n",
        "                                      new_sampling_rate=16000,  # new sample rate for input\n",
        "                                      sample_length_seconds=1  # new length of input in seconds\n",
        "                                      )\n",
        "\n",
        "custom_data_module.setup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61td-GUuUcpA"
      },
      "outputs": [],
      "source": [
        "# Dataloader(s)\n",
        "x = next(iter(custom_data_module.train_dataloader()))\n",
        "y = next(iter(custom_data_module.val_dataloader()))\n",
        "z = next(iter(custom_data_module.test_dataloader()))\n",
        "print('Train Dataloader:')\n",
        "print(torch.tensor(x[0]).shape, len(x[1]))\n",
        "print('Validation Dataloader:')\n",
        "print(torch.tensor(y[0]).shape, len(y[1]))\n",
        "print('Test Dataloader:')\n",
        "print(torch.tensor(z[0]).shape, len(z[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=11,stride=1,padding='valid')\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=9, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
        "        # self.conv5 = nn.Conv1d(in_channels=256, out_channels=64, kernel_size=3, stride=2)\n",
        "        # self.conv6 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        # self.conv7 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=8192, out_features=1024)\n",
        "        # self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=n_classes)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(32,affine=False)\n",
        "        self.bn2 = nn.BatchNorm1d(64,affine=False)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        \n",
        "        # y = x.clone()\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=16,stride=1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv3(x)\n",
        "        # x = x + y\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=4,stride=1)\n",
        "        x = self.conv4(x)\n",
        "        x = F.adaptive_avg_pool1d(x,128)\n",
        "        x = self.bn2(x)\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv5(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv6(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv7(x)\n",
        "\n",
        "        # x = F.avg_pool1d(x,kernel_size=2,stride=1)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, train_loader,valid_loader, optimizer, criterion, device,epochs = 10):\n",
        "\n",
        "    history = {'train':{'loss':[],'accuracy':[]}, 'valid':{'loss':[],'accuracy':[]}}\n",
        "    n = len(train_loader)\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        Loss_epoch = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        evaluation_train = {'accuracy':0, 'loss':0}\n",
        "        for idx, data in enumerate(tqdm(train_loader)):\n",
        "            input, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            Loss_epoch += loss.item()*len(labels)\n",
        "            correct += accuracy(output, labels)*len(labels)\n",
        "            total += len(labels)\n",
        "        # Loss_history.append(Loss_epoch/n)\n",
        "        evaluation_train['accuracy'] = correct/total\n",
        "        evaluation_train['loss'] =  Loss_epoch/total\n",
        "        # evaluation_train = evaluate(model, train_loader,criterion, device)\n",
        "        evaluation_valid,_ = evaluate(model, valid_loader,criterion, device)\n",
        "        print(f'train: {evaluation_train}, valid: {evaluation_valid}')\n",
        "        history['train']['accuracy'].append(evaluation_train['accuracy'])\n",
        "        history['train']['loss'].append(evaluation_train['loss'])\n",
        "\n",
        "\n",
        "\n",
        "        history['valid']['loss'].append(evaluation_valid['loss'])\n",
        "        history['valid']['accuracy'].append(evaluation_valid['accuracy'])\n",
        "    return history\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    _, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
        "\n",
        "def evaluate(model,data_loader, criterion, device,return_preds = False):\n",
        "    model.eval()\n",
        "    Accuracy_history = []\n",
        "    Loss_history = []\n",
        "    PREDS = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            input, target = data[0].to(device), data[1].to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            Accuracy_history.append(accuracy(output, target))\n",
        "            Loss_history.append(loss.item())\n",
        "            if return_preds:\n",
        "                PREDS.extend(torch.max(output, dim=1)[1].tolist())\n",
        "    return {'accuracy': torch.mean(torch.Tensor(Accuracy_history)).item(), 'loss': torch.mean(torch.Tensor(Loss_history)).item()}, PREDS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numel(model(10),only_trainable=True),numel(model(10),only_trainable=False)-numel(model(10),only_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ques2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_attention(q,k,v):\n",
        "    dk = k.shape[-1]\n",
        "    scaled_attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)\n",
        "\n",
        "    attention = F.softmax(scaled_attn, dim=-2)\n",
        "    attention_values = torch.matmul(attention.transpose(-2,-1), v)\n",
        "    return attention_values, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_multiple_heads(d, dk,dv,num_heads):\n",
        "    \"\"\"\n",
        "    Generate multiple heads for the scaled dot-product attention mechanism.\n",
        "    N: tokens length\n",
        "    d: Depth of the input tensor (x: NxD)\n",
        "    dk: Depth of the key,query tensor (w: NxDk)\n",
        "    dv: Depth of the value tensor (W: NxDv)\n",
        "    num_heads: Number of heads to generate\n",
        "\n",
        "    returns: List of [[Wq, Wk,Wv]]*num_heads \n",
        "    \"\"\"\n",
        "    return nn.ModuleList([nn.ModuleList([nn.Linear(d, dk),nn.Linear(d, dk), nn.Linear(d, dv)]) for _ in range(num_heads)])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "class multihead_attention(nn.Module):\n",
        "    def __init__(self, input_length,input_dim, num_heads = 2, dk = 2, dv = 2) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        input_length (N): Length of the input tensor (x: NxD)\n",
        "        input_dim (D): Depth of the input tensor (x: NxD)\n",
        "        \"\"\"\n",
        "        self.num_heads = num_heads\n",
        "        self.input_dim = input_dim\n",
        "        self.input_length = input_length\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.fc = nn.Linear(self.num_heads*self.dv,dv)\n",
        "        self.heads = generate_multiple_heads(self.input_dim,self.dk,self.dv,self.num_heads)\n",
        "\n",
        "    def forward(self,x):\n",
        "        attention_values_all = []\n",
        "        for head in self.heads:\n",
        "            q,k,v = head[0](x),head[1](x),head[2](x)\n",
        "            attention_values, _ = scaled_attention(q,k,v)\n",
        "            attention_values_all.append(attention_values)\n",
        "\n",
        "        attention_values_all = torch.cat(attention_values_all,dim=-1)\n",
        "        return self.fc(attention_values_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(x):\n",
        "\n",
        "    B,N,D = x.shape\n",
        "    positions = torch.arange(N).unsqueeze(-1)\n",
        "    i = torch.arange(D).unsqueeze(-1)\n",
        "    PE = torch.zeros(N,D,requires_grad=False)\n",
        "    w = torch.exp( -2*i*math.log(10000.0)/(D)).squeeze(-1)\n",
        "    PE[0::2,:] = torch.sin(positions[0::2,:]*w)\n",
        "    PE[1::2,:] = torch.cos(positions[1::2,:]*w)\n",
        "    \n",
        "    return x+PE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "class encoder_block(nn.Module):\n",
        "    def __init__(self,input_length,input_dim, num_heads, dk, dv) -> None:\n",
        "        super().__init__()\n",
        "        self.multihead_attention = multihead_attention(input_length = input_length,input_dim=input_dim, num_heads=num_heads, dk = dk, dv=dv)\n",
        "        self.layer_norm1 = nn.LayerNorm(dv)\n",
        "        self.layer_norm2 = nn.LayerNorm(dv)\n",
        "        self.fc = nn.Linear(dv,dv)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        # Postional Encoding\n",
        "        # print(x.shape)\n",
        "        x = positional_encoding(x)\n",
        "        print(x.shape)\n",
        "        # Multihead Attention\n",
        "        y = self.multihead_attention(x)\n",
        "        print(y.shape)\n",
        "        # RESIDUAL CONNECTION\n",
        "        x = x + y\n",
        "\n",
        "        # Layer Normalization\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Feed Forward\n",
        "        y = self.fc(x)\n",
        "\n",
        "        # RESIDUAL CONNECTION\n",
        "        x = x + y\n",
        "        # Layer Normalization\n",
        "        x = self.layer_norm2(x)\n",
        "        # Output\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__(self, input_length,input_dim, num_heads, dk, dv, num_blocks) -> None:\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([encoder_block(input_length,input_dim, num_heads, dk, dv) for _ in range(num_blocks)])\n",
        "    \n",
        "    def forward(self,x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "class transformer(nn.Module):\n",
        "    def __init__(self, n_classes,input_length,input_dim, num_heads, dk, dv, num_blocks) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder(input_length,input_dim, num_heads, dk, dv, num_blocks)\n",
        "        self.fc = nn.Linear(input_length*dv,n_classes)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return F.softmax(x,dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.5685, 0.6491],\n",
              "         [0.5729, 0.6573],\n",
              "         [0.5807, 0.6618]],\n",
              "\n",
              "        [[0.5672, 0.1849],\n",
              "         [0.5400, 0.1859],\n",
              "         [0.5303, 0.1863]],\n",
              "\n",
              "        [[0.5747, 0.5737],\n",
              "         [0.5822, 0.5657],\n",
              "         [0.5753, 0.5798]],\n",
              "\n",
              "        [[0.4937, 0.6526],\n",
              "         [0.4838, 0.6426],\n",
              "         [0.5002, 0.6653]]])"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.manual_seed(0)\n",
        "# B,N,dk = 4,3,2\n",
        "# q = torch.rand(B,N,dk)\n",
        "# k = torch.rand(B,N,dk)\n",
        "# v = torch.rand(B,N,dk)\n",
        "# scaled_attention(q,k,v)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "inp = torch.rand(B,N,dk)\n",
        "target = torch.randint(0,10,(B,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = transformer(10,N,dk,2,dk,2,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.0553, 0.0770, 0.2267, 0.1507, 0.1447, 0.0329, 0.0283, 0.1095, 0.1105,\n",
              "         0.0645],\n",
              "        [0.0553, 0.0770, 0.2267, 0.1507, 0.1447, 0.0329, 0.0283, 0.1095, 0.1105,\n",
              "         0.0645],\n",
              "        [0.0553, 0.0770, 0.2267, 0.1507, 0.1447, 0.0329, 0.0283, 0.1095, 0.1105,\n",
              "         0.0645],\n",
              "        [0.0553, 0.0770, 0.2267, 0.1507, 0.1447, 0.0329, 0.0283, 0.1095, 0.1105,\n",
              "         0.0645]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
