{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4D2_W9OLZON"
      },
      "source": [
        "This code is useful in data loading, visualization and exploration. You are free to modify the code. The code has dependecy on Pytorch Lightning data module. However, you may use Pytorch as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOr0uxaMG_u"
      },
      "source": [
        "**Introduction to Dataset**\n",
        "\n",
        "The data has a total of 10 classes with 40 samples each. Make sure while working with the data, **esc10=True**. In the assignment, you are required to perform 4-fold validation. This dataset has been already divided into 5-folds. The column 'fold' in the metafile denotes the sample in a particular fold. Moreover, first folds is considered for test, rest for 4-fold validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh-C8E00Nic0"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "print('Downlading data... ', end='')\n",
        "# Your code here\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "print('Importing Libraries... ',end='')\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import zipfile\n",
        "from torchaudio.transforms import Resample\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHoVq9fM7EHL"
      },
      "outputs": [],
      "source": [
        "# # Extract data\n",
        "# with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_SDQQJfNK-4"
      },
      "outputs": [],
      "source": [
        "# Getting list of raw audio files\n",
        "wavs = list(path.glob('audio/*'))  # List all audio files in the 'audio' directory using pathlib.Path.glob\n",
        "\n",
        "# Visualizing data\n",
        "waveform, sample_rate = torchaudio.load(wavs[0])  # Load the waveform and sample rate of the first audio file using torchaudio\n",
        "\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))  # Print the shape of the waveform tensor\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))  # Print the sample rate of the audio file\n",
        "\n",
        "# Plot the waveform using matplotlib\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())  # Transpose and convert the waveform tensor to a NumPy array for plotting\n",
        "\n",
        "# Display the audio using IPython.display.Audio\n",
        "ipd.Audio(waveform, rate=sample_rate)  # Create an interactive audio player for the loaded waveform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2JUJdPTRjix"
      },
      "outputs": [],
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Initialize the CustomDataModule with batch size, number of workers, and other parameters\n",
        "        super().__init__()\n",
        "        self.batch_size = kwargs[\"batch_size\"]\n",
        "        self.num_workers = kwargs[\"num_workers\"]\n",
        "        self.data_module_kwargs = kwargs\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Define datasets for training, validation, and testing during Lightning setup\n",
        "\n",
        "        # If in 'fit' or None stage, create training and validation datasets\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.training_dataset = CustomDataset(dataset=\"train\", **self.data_module_kwargs)\n",
        "            self.validation_dataset = CustomDataset(dataset=\"val\", **self.data_module_kwargs)\n",
        "\n",
        "        # If in 'test' or None stage, create testing dataset\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.testing_dataset = CustomDataset(dataset=\"test\", **self.data_module_kwargs)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Return DataLoader for training dataset\n",
        "        return DataLoader(self.training_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # Return DataLoader for validation dataset\n",
        "        return DataLoader(self.validation_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # Return DataLoader for testing dataset\n",
        "        return DataLoader(self.testing_dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def collate_function(self, data):\n",
        "        \"\"\"\n",
        "        Collate function to process a batch of examples and labels.\n",
        "\n",
        "        Args:\n",
        "            data: a tuple of 2 tuples with (example, label) where\n",
        "                example are the split 1 second sub-frame audio tensors per file\n",
        "                label = the label\n",
        "\n",
        "        Returns:\n",
        "            A list containing examples (concatenated tensors) and labels (flattened tensor).\n",
        "        \"\"\"\n",
        "        examples, labels = zip(*data)\n",
        "        # examples = torch.cat(examples)\n",
        "    \n",
        "        examples = torch.stack(examples)\n",
        "        # examples = examples.reshape(examples.size(0),-1,examples.size(-1))\n",
        "        examples = examples.reshape(examples.size(0),1,-1)\n",
        "        labels = torch.flatten(torch.tensor(labels))\n",
        "\n",
        "        return examples, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irm0dFPaS1JR"
      },
      "outputs": [],
      "source": [
        "# Data Setup\n",
        "test_samp = 1 #\"\"\" Do not change this!! \"\"\"\n",
        "valid_samp = 2 # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)\n",
        "batch_size = 32 # Free to change\n",
        "num_workers = 0 # Free to change\n",
        "custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                      num_workers=num_workers,\n",
        "                                      data_directory=path,\n",
        "                                      data_frame=df,\n",
        "                                      validation_fold=valid_samp,\n",
        "                                      testing_fold=test_samp,  # set to 0 for no test set\n",
        "                                      esc_10_flag=True,\n",
        "                                      file_column='filename',\n",
        "                                      label_column='category',\n",
        "                                      sampling_rate=44100,\n",
        "                                      new_sampling_rate=16000,  # new sample rate for input\n",
        "                                      sample_length_seconds=1  # new length of input in seconds\n",
        "                                      )\n",
        "\n",
        "custom_data_module.setup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW9GhLFjUayx"
      },
      "outputs": [],
      "source": [
        "# Data Exploration\n",
        "print('Class Label: ', custom_data_module.training_dataset[5][1])  # this prints the class label\n",
        "print('Shape of data sample tensor: ', custom_data_module.training_dataset[5][0].shape)  # this prints the shape of the sample (Frames, Channel, Features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61td-GUuUcpA"
      },
      "outputs": [],
      "source": [
        "# Dataloader(s)\n",
        "x = next(iter(custom_data_module.train_dataloader()))\n",
        "y = next(iter(custom_data_module.val_dataloader()))\n",
        "z = next(iter(custom_data_module.test_dataloader()))\n",
        "print('Train Dataloader:')\n",
        "print(torch.tensor(x[0]).shape, len(x[1]))\n",
        "print('Validation Dataloader:')\n",
        "print(torch.tensor(y[0]).shape, len(y[1]))\n",
        "print('Test Dataloader:')\n",
        "print(torch.tensor(z[0]).shape, len(z[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = custom_data_module.train_dataloader()\n",
        "valid_loader = custom_data_module.val_dataloader()\n",
        "test_loader = custom_data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=11,stride=1,padding='valid')\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=9, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
        "        # self.conv5 = nn.Conv1d(in_channels=256, out_channels=64, kernel_size=3, stride=2)\n",
        "        # self.conv6 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        # self.conv7 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=8192, out_features=1024)\n",
        "        # self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=n_classes)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(32,affine=False)\n",
        "        self.bn2 = nn.BatchNorm1d(64,affine=False)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        \n",
        "        # y = x.clone()\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=16,stride=1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv3(x)\n",
        "        # x = x + y\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x,kernel_size=4,stride=1)\n",
        "        x = self.conv4(x)\n",
        "        x = F.adaptive_avg_pool1d(x,128)\n",
        "        x = self.bn2(x)\n",
        "        x = F.dropout1d(x,0.1)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv5(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv6(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.max_pool1d(x,kernel_size=2,stride=1)\n",
        "        # x = self.conv7(x)\n",
        "\n",
        "        # x = F.avg_pool1d(x,kernel_size=2,stride=1)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, train_loader,valid_loader, optimizer, criterion, device,epochs = 10):\n",
        "\n",
        "    history = {'train':{'loss':[],'accuracy':[]}, 'valid':{'loss':[],'accuracy':[]}}\n",
        "    n = len(train_loader)\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        Loss_epoch = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        evaluation_train = {'accuracy':0, 'loss':0}\n",
        "        for idx, data in enumerate(tqdm(train_loader)):\n",
        "            input, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            Loss_epoch += loss.item()*len(labels)\n",
        "            correct += accuracy(output, labels)*len(labels)\n",
        "            total += len(labels)\n",
        "        # Loss_history.append(Loss_epoch/n)\n",
        "        evaluation_train['accuracy'] = correct/total\n",
        "        evaluation_train['loss'] =  Loss_epoch/total\n",
        "        # evaluation_train = evaluate(model, train_loader,criterion, device)\n",
        "        evaluation_valid,_ = evaluate(model, valid_loader,criterion, device)\n",
        "        print(f'train: {evaluation_train}, valid: {evaluation_valid}')\n",
        "        history['train']['accuracy'].append(evaluation_train['accuracy'])\n",
        "        history['train']['loss'].append(evaluation_train['loss'])\n",
        "\n",
        "\n",
        "\n",
        "        history['valid']['loss'].append(evaluation_valid['loss'])\n",
        "        history['valid']['accuracy'].append(evaluation_valid['accuracy'])\n",
        "    return history\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    _, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
        "\n",
        "def evaluate(model,data_loader, criterion, device,return_preds = False):\n",
        "    model.eval()\n",
        "    Accuracy_history = []\n",
        "    Loss_history = []\n",
        "    PREDS = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            input, target = data[0].to(device), data[1].to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            Accuracy_history.append(accuracy(output, target))\n",
        "            Loss_history.append(loss.item())\n",
        "            if return_preds:\n",
        "                PREDS.extend(torch.max(output, dim=1)[1].tolist())\n",
        "    return {'accuracy': torch.mean(torch.Tensor(Accuracy_history)).item(), 'loss': torch.mean(torch.Tensor(Loss_history)).item()}, PREDS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def numel(m: torch.nn.Module, only_trainable: bool = False):\n",
        "    \"\"\"\n",
        "    Returns the total number of parameters used by `m` (only counting\n",
        "    shared parameters once); if `only_trainable` is True, then only\n",
        "    includes parameters with `requires_grad = True`\n",
        "    \"\"\"\n",
        "    parameters = list(m.parameters())\n",
        "    if only_trainable:\n",
        "        parameters = [p for p in parameters if p.requires_grad]\n",
        "    unique = {p.data_ptr(): p for p in parameters}.values()\n",
        "    return sum(p.numel() for p in unique)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numel(model(10),only_trainable=True),numel(model(10),only_trainable=False)-numel(model(10),only_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "n_classes = len(custom_data_module.training_dataset.categories)# number of classes = 10\n",
        "model_train = model(n_classes)\n",
        "history = train(model_train, train_loader, valid_loader, optim.Adam(model_train.parameters(), lr=0.001), nn.CrossEntropyLoss(), 'cuda', epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ques2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
