# -*- coding: utf-8 -*-
"""M23CSA003.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MP7EKCYzYoPvFX9YYOQbUbq93W6kf5Yp
"""

# !pip install lightning
# !pip install wandb

# Importing Libraries
print("Importing Libraries... ", end="")
import os
from pathlib import Path
import pandas as pd
import torchaudio
import zipfile
from torchaudio.transforms import Resample
import IPython.display as ipd
from matplotlib import pyplot as plt
from tqdm import tqdm
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
import torch

import wandb

import numpy as np
import sklearn.metrics as skm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
import math

print("Done")

# from google.colab import drive
# drive.mount('/content/drive')

# # Extract data
# with zipfile.ZipFile("/content/drive/MyDrive/Deep Learning/A2/data.zip", 'r') as zip_ref:
#     zip_ref.extractall("/content/")

# Loading dataset
path = Path("./data")
df = pd.read_csv("./data/meta/esc50.csv")

torchaudio.list_audio_backends()

# torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False
# torchaudio.set_audio_backend("soundfile")

# Getting list of raw audio files
wavs = list(
    path.glob("audio/*")
)  # List all audio files in the 'audio' directory using pathlib.Path.glob

# Visualizing data
waveform, sample_rate = torchaudio.load(
    str(wavs[17])
)  # Load the waveform and sample rate of the first audio file using torchaudio

print(
    "Shape of waveform: {}".format(waveform.size())
)  # Print the shape of the waveform tensor
print(
    "Sample rate of waveform: {}".format(sample_rate)
)  # Print the sample rate of the audio file

# Plot the waveform using matplotlib
plt.figure()
plt.plot(
    waveform.t().numpy()
)  # Transpose and convert the waveform tensor to a NumPy array for plotting

# Display the audio using IPython.display.Audio
ipd.Audio(
    waveform, rate=sample_rate
)  # Create an interactive audio player for the loaded waveform


class CustomDataset(Dataset):
    def __init__(self, dataset, **kwargs):
        # Initialize CustomDataset object with relevant parameters
        # dataset: "train", "val", or "test"
        # kwargs: Additional parameters like data directory, dataframe, folds, etc.

        # Extract parameters from kwargs
        self.data_directory = kwargs["data_directory"]
        self.data_frame = kwargs["data_frame"]
        self.validation_fold = kwargs["validation_fold"]
        self.testing_fold = kwargs["testing_fold"]
        self.esc_10_flag = kwargs["esc_10_flag"]
        self.file_column = kwargs["file_column"]
        self.label_column = kwargs["label_column"]
        self.sampling_rate = kwargs["sampling_rate"]
        self.new_sampling_rate = kwargs["new_sampling_rate"]
        self.sample_length_seconds = kwargs["sample_length_seconds"]

        # Filter dataframe based on esc_10_flag and data_type
        if self.esc_10_flag:
            self.data_frame = self.data_frame.loc[self.data_frame["esc10"] == True]

        if dataset == "train":
            self.data_frame = self.data_frame.loc[
                (self.data_frame["fold"] != self.validation_fold)
                & (self.data_frame["fold"] != self.testing_fold)
            ]
        elif dataset == "val":
            self.data_frame = self.data_frame.loc[
                self.data_frame["fold"] == self.validation_fold
            ]
        elif dataset == "test":
            self.data_frame = self.data_frame.loc[
                self.data_frame["fold"] == self.testing_fold
            ]

        # Get unique categories from the filtered dataframe
        self.categories = sorted(self.data_frame[self.label_column].unique())

        # Initialize lists to hold file names, labels, and folder numbers
        self.file_names = []
        self.labels = []

        # Initialize dictionaries for category-to-index and index-to-category mapping
        self.category_to_index = {}
        self.index_to_category = {}

        for i, category in enumerate(self.categories):
            self.category_to_index[category] = i
            self.index_to_category[i] = category

        # Populate file names and labels lists by iterating through the dataframe
        for ind in tqdm(range(len(self.data_frame))):
            row = self.data_frame.iloc[ind]
            file_path = self.data_directory / "audio" / row[self.file_column]
            self.file_names.append(file_path)
            self.labels.append(self.category_to_index[row[self.label_column]])

        self.resampler = torchaudio.transforms.Resample(
            self.sampling_rate, self.new_sampling_rate
        )

        # Window size for rolling window sample splits (unfold method)
        if self.sample_length_seconds == 2:
            self.window_size = self.new_sampling_rate * 2
            self.step_size = int(self.new_sampling_rate * 0.75)
        else:
            self.window_size = self.new_sampling_rate
            self.step_size = int(self.new_sampling_rate * 0.5)

    def __getitem__(self, index):
        # Split audio files with overlap, pass as stacked tensors tensor with a single label
        path = self.file_names[index]
        audio_file = torchaudio.load(str(path), format=None, normalize=True)
        audio_tensor = self.resampler(audio_file[0])
        splits = audio_tensor.unfold(1, self.window_size, self.step_size)
        samples = splits.permute(1, 0, 2)
        return samples, self.labels[index]

    def __len__(self):
        return len(self.file_names)


class CustomDataModule(pl.LightningDataModule):
    def __init__(self, **kwargs):
        # Initialize the CustomDataModule with batch size, number of workers, and other parameters
        super().__init__()
        self.batch_size = kwargs["batch_size"]
        self.num_workers = kwargs["num_workers"]
        self.data_module_kwargs = kwargs

    def setup(self, stage=None):
        # Define datasets for training, validation, and testing during Lightning setup

        # If in 'fit' or None stage, create training and validation datasets
        if stage == "fit" or stage is None:
            self.training_dataset = CustomDataset(
                dataset="train", **self.data_module_kwargs
            )
            self.validation_dataset = CustomDataset(
                dataset="val", **self.data_module_kwargs
            )

        # If in 'test' or None stage, create testing dataset
        if stage == "test" or stage is None:
            self.testing_dataset = CustomDataset(
                dataset="test", **self.data_module_kwargs
            )

    def train_dataloader(self):
        # Return DataLoader for training dataset
        return DataLoader(
            self.training_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            collate_fn=self.collate_function,
            num_workers=self.num_workers,
        )

    def val_dataloader(self):
        # Return DataLoader for validation dataset
        return DataLoader(
            self.validation_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            collate_fn=self.collate_function,
            num_workers=self.num_workers,
        )

    def test_dataloader(self):
        # Return DataLoader for testing dataset
        return DataLoader(
            self.testing_dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=self.collate_function,
            num_workers=self.num_workers,
        )

    def collate_function(self, data):
        """
        Collate function to process a batch of examples and labels.

        Args:
            data: a tuple of 2 tuples with (example, label) where
                example are the split 1 second sub-frame audio tensors per file
                label = the label

        Returns:
            A list containing examples (concatenated tensors) and labels (flattened tensor).
        """
        examples, labels = zip(*data)
        # examples = torch.cat(examples)

        examples = torch.stack(examples)
        # examples = examples.reshape(examples.size(0),-1,examples.size(-1))
        examples = examples.reshape(examples.size(0), 1, -1)
        labels = torch.flatten(torch.tensor(labels))

        return examples, labels


def train_valid_test_loader(valid_fold=2):
    # Data Setup
    test_samp = 1  # """ Do not change this!! """
    valid_samp = (
        2  # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)
    )
    batch_size = 32  # Free to change
    num_workers = 0  # Free to change
    custom_data_module = CustomDataModule(
        batch_size=batch_size,
        num_workers=num_workers,
        data_directory=path,
        data_frame=df,
        validation_fold=valid_samp,
        testing_fold=test_samp,  # set to 0 for no test set
        esc_10_flag=True,
        file_column="filename",
        label_column="category",
        sampling_rate=44100,
        new_sampling_rate=16000,  # new sample rate for input
        sample_length_seconds=1,  # new length of input in seconds
    )

    custom_data_module.setup()
    train_loader = custom_data_module.train_dataloader()
    valid_loader = custom_data_module.val_dataloader()
    test_loader = custom_data_module.test_dataloader()

    return (
        train_loader,
        valid_loader,
        test_loader,
        custom_data_module.testing_dataset.labels,
        custom_data_module.training_dataset.categories,
    )


def numel(m: torch.nn.Module, only_trainable: bool = False):
    """
    Returns the total number of parameters used by `m` (only counting
    shared parameters once); if `only_trainable` is True, then only
    includes parameters with `requires_grad = True`
    """
    parameters = list(m.parameters())
    if only_trainable:
        parameters = [p for p in parameters if p.requires_grad]
    unique = {p.data_ptr(): p for p in parameters}.values()
    return sum(p.numel() for p in unique)


def train(
    model,
    train_loader,
    valid_loader,
    optimizer,
    criterion,
    device,
    epochs=10,
    log_wb=False,
):

    history = {
        "train": {"loss": [], "accuracy": []},
        "valid": {"loss": [], "accuracy": []},
    }
    n = len(train_loader)
    model.to(device)
    for epoch in range(epochs):
        model.train()
        Loss_epoch = 0
        correct = 0
        total = 0
        evaluation_train = {"accuracy": 0, "loss": 0}
        for idx, data in enumerate(tqdm(train_loader)):
            input, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            output = model(input)
            loss = criterion(output, labels)
            loss.backward()
            optimizer.step()

            Loss_epoch += loss.item() * len(labels)
            correct += accuracy(output, labels) * len(labels)
            total += len(labels)
        # Loss_history.append(Loss_epoch/n)
        evaluation_train["accuracy"] = correct / total
        evaluation_train["loss"] = Loss_epoch / total
        # evaluation_train = evaluate(model, train_loader,criterion, device)
        evaluation_valid, _ = evaluate(model, valid_loader, criterion, device)
        print(f"epoch: {epoch}, train: {evaluation_train}, valid: {evaluation_valid}")

        history["train"]["accuracy"].append(evaluation_train["accuracy"])
        history["train"]["loss"].append(evaluation_train["loss"])
        history["valid"]["loss"].append(evaluation_valid["loss"])
        history["valid"]["accuracy"].append(evaluation_valid["accuracy"])

        if log_wb:
            wandb.log(
                {
                    "train/train_loss": evaluation_train["loss"],
                    "train/train_accuracy": evaluation_train["accuracy"],
                    "val/valid_loss": evaluation_valid["loss"],
                    "val/valid_accuracy": evaluation_valid["accuracy"],
                }
            )

    return history


def accuracy(output, labels):
    _, preds = torch.max(output, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()


def evaluate(model, data_loader, criterion, device, return_preds=False):
    model.eval()
    Accuracy_history = []
    Loss_history = []
    PREDS = []
    with torch.no_grad():
        for idx, data in enumerate(data_loader):
            input, target = data[0].to(device), data[1].to(device)
            output = model(input)
            loss = criterion(output, target)
            Accuracy_history.append(accuracy(output, target))
            Loss_history.append(loss.item())
            if return_preds:
                # PREDS.extend(torch.max(output, dim=1)[1].tolist())
                PREDS.extend(output.tolist())

    return {
        "accuracy": torch.mean(torch.Tensor(Accuracy_history)).item(),
        "loss": torch.mean(torch.Tensor(Loss_history)).item(),
    }, PREDS


"""# Q1"""


class model(nn.Module):
    def __init__(self, n_classes, dropout=0.1, fc_size=1024):
        super().__init__()
        self.conv1 = nn.Conv1d(
            in_channels=1, out_channels=16, kernel_size=7, stride=1, padding="valid"
        )
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1)
        self.conv3 = nn.Conv1d(in_channels=32, out_channels=48, kernel_size=3, stride=1)
        self.conv4 = nn.Conv1d(in_channels=48, out_channels=64, kernel_size=3, stride=1)

        self.fc1 = nn.Linear(in_features=8192, out_features=fc_size)
        self.fc = nn.Linear(in_features=fc_size, out_features=n_classes)

        self.bn1 = nn.BatchNorm1d(32, affine=False)
        self.bn2 = nn.BatchNorm1d(64, affine=False)
        self.dropout_rate = dropout

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)

        # y = x.clone()
        x = F.dropout1d(x, self.dropout_rate)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool1d(x, kernel_size=16, stride=1)
        x = self.bn1(x)
        x = self.conv3(x)
        # x = x + y
        x = F.relu(x)
        x = F.max_pool1d(x, kernel_size=4, stride=1)
        x = self.conv4(x)
        x = F.adaptive_avg_pool1d(x, 128)
        x = self.bn2(x)
        x = F.dropout1d(x, self.dropout_rate)

        x = x.flatten(start_dim=1)
        x = self.fc1(x)
        x = F.relu(x)

        x = self.fc(x)
        x = F.softmax(x, dim=1)
        return x


def reset_wandb_env():
    exclude = {
        "WANDB_PROJECT",
        "WANDB_ENTITY",
        "WANDB_API_KEY",
    }
    for key in os.environ.keys():
        if key.startswith("WANDB_") and key not in exclude:
            del os.environ[key]


def train_sweep_arch1(num, sweep_id, sweep_run_name, config):
    run_name = f"{sweep_run_name}-{num}"
    run = wandb.init(
        group=sweep_id,
        job_type=sweep_run_name,
        name=run_name,
        config=config,
        reinit=True,
    )

    config = wandb.config

    train_loader, valid_loader, test_loader, ground_truth, class_names = (
        train_valid_test_loader(valid_fold=num)
    )

    model_train = model(10, dropout=config.dropout, fc_size=config.fc_size)

    trainable_params = numel(model_train, only_trainable=True)
    non_trainable_params = numel(model_train, only_trainable=False) - trainable_params
    print(
        f"trainiable_params: {trainable_params}, non_trainable_params: {non_trainable_params}, total_params: {trainable_params+non_trainable_params}"
    )
    print("val_fold: ", num)

    wandb.config.update(
        {
            "trainable_params": trainable_params,
            "non_trainable_params": non_trainable_params,
            "total_params": trainable_params + non_trainable_params,
        }
    )
    if config.optimizer == "adam":
        optimizer = optim.Adam(model_train.parameters(), lr=config.learning_rate)
    elif config.optimizer == "sgd":
        optimizer = optim.SGD(model_train.parameters(), lr=config.learning_rate)

    history = train(
        model=model_train,
        train_loader=train_loader,
        valid_loader=valid_loader,
        optimizer=optimizer,
        criterion=nn.CrossEntropyLoss(),
        device="cuda",
        epochs=config.epochs,
        log_wb=True,
    )

    test_eval, test_preds = evaluate(
        model_train, test_loader, nn.CrossEntropyLoss(), "cuda", return_preds=True
    )
    run.summary["test_accuracy"] = test_eval["accuracy"]
    run.summary["test_loss"] = test_eval["loss"]
    # plots
    test_preds = np.array(test_preds)
    # # Precision Recall
    run.log({"pr": wandb.plot.pr_curve(ground_truth, test_preds, labels=class_names)})

    # ROC
    run.log({"roc": wandb.plot.roc_curve(ground_truth, test_preds, labels=class_names)})

    # Confusion Matrix
    cm = wandb.plot.confusion_matrix(
        y_true=ground_truth, preds=test_preds.argmax(axis=1), class_names=class_names
    )
    wandb.log({"conf_mat": cm})

    # precison, recall, F1 Score
    run.summary["f1"] = skm.f1_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )
    run.summary["precision"] = skm.precision_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )
    run.summary["recall"] = skm.recall_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )

    history.update(
        {"test_accuracy": test_eval["accuracy"], "test_loss": test_eval["loss"]}
    )
    print(f"test_accuracy: {test_eval['accuracy']}, test_loss: {test_eval['loss']}")

    run.finish()
    return history


"""# Q2"""


class cnn_features(nn.Module):
    def __init__(self, max_length, input_dim):
        super().__init__()
        self.conv1 = nn.Conv1d(
            in_channels=1, out_channels=32, kernel_size=11, stride=1, padding="valid"
        )
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=9, stride=1)
        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=1)
        self.conv4 = nn.Conv1d(
            in_channels=32, out_channels=max_length, kernel_size=3, stride=1
        )

        self.bn1 = nn.BatchNorm1d(32, affine=False)
        self.bn2 = nn.BatchNorm1d(max_length, affine=False)
        self.max_length = max_length
        self.input_dim = input_dim

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)

        x = F.dropout1d(x, 0.1)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool1d(x, kernel_size=16, stride=1)
        x = self.bn1(x)
        x = self.conv3(x)
        # x = x + y
        x = F.relu(x)
        x = F.max_pool1d(x, kernel_size=4, stride=1)
        x = self.conv4(x)
        x = F.adaptive_avg_pool1d(x, self.input_dim)
        x = self.bn2(x)

        # print(x.shape)
        return x


def scaled_attention(q, k, v):
    dk = k.shape[-1]
    scaled_attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)

    attention = F.softmax(scaled_attn, dim=-2)
    attention_values = torch.matmul(attention.transpose(-2, -1), v)
    return attention_values, attention


def generate_multiple_heads(d, dk, dv, num_heads):
    """
    Generate multiple heads for the scaled dot-product attention mechanism.
    returns: List of [[Wq, Wk,Wv]]*num_heads
    """
    return nn.ModuleList(
        [
            nn.ModuleList([nn.Linear(d, dk), nn.Linear(d, dk), nn.Linear(d, dv)])
            for _ in range(num_heads)
        ]
    )


class multihead_attention(nn.Module):
    def __init__(self, input_length, input_dim, num_heads=2, dk=2, dv=2) -> None:
        super().__init__()

        self.num_heads = num_heads
        self.input_dim = input_dim
        self.input_length = input_length
        self.dk = dk
        self.dv = dv
        self.fc = nn.Linear(self.num_heads * self.dv, dv)
        self.heads = generate_multiple_heads(
            self.input_dim, self.dk, self.dv, self.num_heads
        )

    def forward(self, x):
        attention_values_all = []
        for head in self.heads:
            q, k, v = head[0](x), head[1](x), head[2](x)
            attention_values, _ = scaled_attention(q, k, v)
            attention_values_all.append(attention_values)

        attention_values_all = torch.cat(attention_values_all, dim=-1)
        return self.fc(attention_values_all)


class encoder_block(nn.Module):
    def __init__(self, input_length, input_dim, num_heads, dk, dv) -> None:
        super().__init__()
        self.multihead_attention = multihead_attention(
            input_length=input_length,
            input_dim=input_dim,
            num_heads=num_heads,
            dk=dk,
            dv=dv,
        )
        self.layer_norm1 = nn.LayerNorm(dv)
        self.layer_norm2 = nn.LayerNorm(dv)
        self.fc = nn.Linear(dv, dv)

    def forward(self, x):
        # Postional Encoding
        # print(x.shape)
        # print(x.shape)
        x = self.positional_encoding(x)

        # print(x.shape)
        # Multihead Attention
        y = self.multihead_attention(x)
        # print(y.shape)
        # RESIDUAL CONNECTION
        x = x + y

        # Layer Normalization
        x = self.layer_norm1(x)

        # Feed Forward
        y = self.fc(x)

        # RESIDUAL CONNECTION
        x = x + y
        # Layer Normalization
        x = self.layer_norm2(x)
        # Output
        return x

    def positional_encoding(self, x):

        B, N, D = x.shape
        # N = N-1
        positions = torch.arange(1, N).unsqueeze(-1)
        i = torch.arange(D).unsqueeze(-1)
        PE = torch.zeros(N, D, requires_grad=False).to(x.device)
        w = torch.exp(-2 * i * math.log(10000.0) / (D)).squeeze(-1)
        PE[1::2, :] = torch.sin(positions[0::2, :] * w)
        PE[2::2, :] = torch.cos(positions[1::2, :] * w)
        # PE = torch.cat([torch.zeros(PE.size(0), 1, D).to(x.device), PE], dim=1)
        return x + PE


class encoder(nn.Module):
    def __init__(self, input_length, input_dim, num_heads, dk, dv, num_blocks) -> None:
        super().__init__()
        self.blocks = nn.ModuleList(
            [
                encoder_block(input_length, input_dim, num_heads, dk, dv)
                for _ in range(num_blocks)
            ]
        )

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x


class transformer(nn.Module):
    def __init__(
        self, n_classes, features_length, input_dim, num_heads, dk, dv, num_blocks
    ) -> None:
        super().__init__()

        """
        input_length/feature_length (N): Length of the input tensor (x: NxD)
        input_dim (D): Depth of the input tensor (x: NxD)
        dk: Depth of the key,query tensor (w: NxDk)
        dv: Depth of the value tensor (W: NxDv)
        num_heads: Number of heads to generate
        num_blocks: Number of encoder blocks to use
        n_classes: Number of classes to predict
        """

        input_length = features_length
        self.cnn_features = cnn_features(input_length, input_dim)
        input_length += 1
        self.encoder = encoder(input_length, input_dim, num_heads, dk, dv, num_blocks)
        self.fc = nn.Linear(dv, n_classes)

    def forward(self, x):
        x = self.cnn_features(x)
        # print(x.shape,"cnn")

        cls_token = torch.zeros(x.size(0), 1, x.size(-1)).to(x.device)
        x = torch.cat([cls_token, x], dim=1)

        x = self.encoder(x)
        x = x[:, 0]
        # x = x.flatten(start_dim=1)

        x = self.fc(x)
        return F.softmax(x, dim=-1)


def train_sweep_arch2(num, sweep_id, sweep_run_name, config):
    run_name = f"{sweep_run_name}-{num}"
    run = wandb.init(
        group=sweep_id,
        job_type=sweep_run_name,
        name=run_name,
        config=config,
        reinit=True,
    )

    config = wandb.config
    train_loader, valid_loader, test_loader, ground_truth, class_names = (
        train_valid_test_loader(valid_fold=num)
    )

    model_train = transformer(
        n_classes=10,
        features_length=config.features_length,
        input_dim=config.dv,
        num_heads=config.num_heads,
        dk=config.dk,
        dv=config.dv,
        num_blocks=config.num_blocks,
    )

    trainable_params = numel(model_train, only_trainable=True)
    non_trainable_params = numel(model_train, only_trainable=False) - trainable_params
    print(
        f"trainiable_params: {trainable_params}, non_trainable_params: {non_trainable_params}, total_params: {trainable_params+non_trainable_params}"
    )
    print("val_fold: ", num)

    wandb.config.update(
        {
            "trainable_params": trainable_params,
            "non_trainable_params": non_trainable_params,
            "total_params": trainable_params + non_trainable_params,
        }
    )
    if config.optimizer == "adam":
        optimizer = optim.Adam(model_train.parameters(), lr=config.learning_rate)
    elif config.optimizer == "sgd":
        optimizer = optim.SGD(model_train.parameters(), lr=config.learning_rate)

    history = train(
        model=model_train,
        train_loader=train_loader,
        valid_loader=valid_loader,
        optimizer=optimizer,
        criterion=nn.CrossEntropyLoss(),
        device="cuda",
        epochs=config.epochs,
        log_wb=True,
    )

    test_eval, test_preds = evaluate(
        model_train, test_loader, nn.CrossEntropyLoss(), "cuda", True
    )
    run.summary["test_accuracy"] = test_eval["accuracy"]
    run.summary["test_loss"] = test_eval["loss"]
    # plots
    test_preds = np.array(test_preds)
    # # Precision Recall
    run.log({"pr": wandb.plot.pr_curve(ground_truth, test_preds, labels=class_names)})

    # ROC
    run.log({"roc": wandb.plot.roc_curve(ground_truth, test_preds, labels=class_names)})

    # Confusion Matrix
    cm = wandb.plot.confusion_matrix(
        y_true=ground_truth, preds=test_preds.argmax(axis=1), class_names=class_names
    )
    wandb.log({"conf_mat": cm})

    # precison, recall, F1 Score
    run.summary["f1"] = skm.f1_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )
    run.summary["precision"] = skm.precision_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )
    run.summary["recall"] = skm.recall_score(
        ground_truth, test_preds.argmax(axis=1), average="weighted"
    )

    history.update(
        {"test_accuracy": test_eval["accuracy"], "test_loss": test_eval["loss"]}
    )
    print(f"test_accuracy: {test_eval['accuracy']}, test_loss: {test_eval['loss']}")

    run.finish()
    return history


"""# Run Models"""


class Cross_validate:
    def __init__(self, arch) -> None:
        self.arch = arch

    def cross_validate(self):
        sweep_run = wandb.init()
        sweep_id = sweep_run.sweep_id or "unknown"
        sweep_url = sweep_run.get_sweep_url()
        project_url = sweep_run.get_project_url()
        sweep_group_url = f"{project_url}/groups/{sweep_id}"
        sweep_run.notes = sweep_group_url
        sweep_run.save()
        sweep_run_name = sweep_run.name or sweep_run.id or "unknown_2"
        sweep_run_id = sweep_run.id
        sweep_run.finish()
        wandb.sdk.wandb_setup._setup(_reset=True)

        avg_train_acc = []
        avg_train_loss = []
        avg_valid_acc = []
        avg_valid_loss = []
        avg_test_acc = []
        avg_test_loss = []

        for val_fold in [2, 3, 4, 5]:

            reset_wandb_env()
            if self.arch == "arch1":
                history = train_sweep_arch1(
                    sweep_id=sweep_id,
                    num=val_fold,
                    sweep_run_name=sweep_run_name,
                    config=dict(sweep_run.config),
                )
            elif self.arch == "arch2":
                history = train_sweep_arch2(
                    sweep_id=sweep_id,
                    num=val_fold,
                    sweep_run_name=sweep_run_name,
                    config=dict(sweep_run.config),
                )

            avg_train_acc.append(history["train"]["accuracy"][-1])
            avg_train_loss.append(history["train"]["loss"][-1])
            avg_valid_acc.append(history["valid"]["accuracy"][-1])
            avg_valid_loss.append(history["valid"]["loss"][-1])
            avg_test_acc.append(history["test_accuracy"])
            avg_test_loss.append(history["test_loss"])

            # resume the sweep run
        sweep_run = wandb.init(id=sweep_run_id, resume="must")
        # log metric to sweep run
        # sweep_run.log(dict(val_accuracy=sum(metrics) / len(metrics)))
        sweep_run.summary["avg_train_accuracy"] = sum(avg_train_acc) / len(
            avg_train_acc
        )
        sweep_run.summary["avg_train_loss"] = sum(avg_train_loss) / len(avg_train_loss)
        sweep_run.summary["avg_valid_accuracy"] = sum(avg_valid_acc) / len(
            avg_valid_acc
        )
        sweep_run.summary["avg_valid_loss"] = sum(avg_valid_loss) / len(avg_valid_loss)
        sweep_run.summary["avg_test_accuracy"] = sum(avg_test_acc) / len(avg_test_acc)
        sweep_run.summary["avg_test_loss"] = sum(avg_test_loss) / len(avg_test_loss)
        sweep_run.summary["best_test_accuracy"] = max(avg_test_acc)
        sweep_run.finish()

        print("*" * 40)
        print("Sweep URL:       ", sweep_url)
        print("Sweep Group URL: ", sweep_group_url)
        print("*" * 40)


"""# Run Arch 1"""

sweep_config_arch1 = {
    "method": "random",
    "metric": {"goal": "maximize", "name": "avg_valid_accuracy"},
    "parameters": {
        "dropout": {"values": [0.2, 0.4, 0.5]},
        "epochs": {"value": 100},
        # "arch":{ "value": "arch1"},
        "fc_size": {"values": [128, 256, 512, 1024]},
        "learning_rate": {"distribution": "uniform", "max": 0.01, "min": 0},
        "optimizer": {"values": ["adam", "sgd"]},
    },
}

sweep_id = wandb.sweep(sweep_config_arch1, project="DL_A2_Arch1")
wandb.agent(sweep_id, function=Cross_validate(arch="arch1").cross_validate, count=2)

wandb.finish()

"""# Run Arch 2"""


for num_heads in [1, 2, 4]:
    sweep_config_arch2 = {
        "method": "random",
        "metric": {"goal": "maximize", "name": "avg_valid_accuracy"},
        "parameters": {
            "features_length": {"values": [16, 32]},
            "epochs": {"value": 100},
            # "arch":{ "value": "arch1"},
            "num_heads": {"value": num_heads},
            "dk": {"values": [32, 64]},
            "dv": {"values": [32, 64]},
            "num_blocks": {"values": [2, 3]},
            "learning_rate": {"distribution": "uniform", "max": 0.01, "min": 0},
            "optimizer": {"values": ["adam", "sgd"]},
        },
    }
    sweep_id = wandb.sweep(sweep_config_arch2, project="DL_A2_Arch2_cls_final")
    wandb.agent(sweep_id, function=Cross_validate(arch="arch2").cross_validate, count=2)

    wandb.finish()
